{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case Study - Whats Cooking (Kaggle Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Datafiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cuisine     id                                        ingredients\n",
      "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
      "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
      "      id                                        ingredients\n",
      "0  18009  [baking powder, eggs, all-purpose flour, raisi...\n",
      "1  28583  [sugar, egg yolks, corn starch, cream of tarta...\n"
     ]
    }
   ],
   "source": [
    "print(train.head(2))\n",
    "print(test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing space within words from the ingredients column \n",
    "new = []\n",
    "for items in train.ingredients:\n",
    "    items = [item.replace(' ', '') for item in items]\n",
    "    new.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = []\n",
    "for items in test.ingredients:\n",
    "    items = [item.replace(' ', '') for item in items]\n",
    "    new_test.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['romainelettuce', 'blackolives', 'grapetomatoes', 'garlic', 'pepper', 'purpleonion', 'seasoning', 'garbanzobeans', 'fetacheesecrumbles'], ['plainflour', 'groundpepper', 'salt', 'tomatoes', 'groundblackpepper', 'thyme', 'eggs', 'greentomatoes', 'yellowcornmeal', 'milk', 'vegetableoil'], ['eggs', 'pepper', 'salt', 'mayonaise', 'cookingoil', 'greenchilies', 'grilledchickenbreasts', 'garlicpowder', 'yellowonion', 'soysauce', 'butter', 'chickenlivers'], ['water', 'vegetableoil', 'wheat', 'salt'], ['blackpepper', 'shallots', 'cornflour', 'cayennepepper', 'onions', 'garlicpaste', 'milk', 'butter', 'salt', 'lemonjuice', 'water', 'chilipowder', 'passata', 'oil', 'groundcumin', 'bonelesschickenskinlessthigh', 'garammasala', 'doublecream', 'naturalyogurt', 'bayleaf'], ['plainflour', 'sugar', 'butter', 'eggs', 'freshgingerroot', 'salt', 'groundcinnamon', 'milk', 'vanillaextract', 'groundginger', 'powderedsugar', 'bakingpowder'], ['oliveoil', 'salt', 'mediumshrimp', 'pepper', 'garlic', 'choppedcilantro', 'jalapenochilies', 'flatleafparsley', 'skirtsteak', 'whitevinegar', 'seasalt', 'bayleaf', 'chorizosausage'], ['sugar', 'pistachionuts', 'whitealmondbark', 'flour', 'vanillaextract', 'oliveoil', 'almondextract', 'eggs', 'bakingpowder', 'driedcranberries'], ['oliveoil', 'purpleonion', 'freshpineapple', 'pork', 'poblanopeppers', 'corntortillas', 'cheddarcheese', 'groundblackpepper', 'salt', 'iceberglettuce', 'lime', 'jalapenochilies', 'choppedcilantrofresh'], ['choppedtomatoes', 'freshbasil', 'garlic', 'extra-virginoliveoil', 'koshersalt', 'flatleafparsley']]\n",
      "                                                   0\n",
      "0  [romainelettuce, blackolives, grapetomatoes, g...\n",
      "1  [plainflour, groundpepper, salt, tomatoes, gro...\n",
      "2  [eggs, pepper, salt, mayonaise, cookingoil, gr...\n",
      "3                 [water, vegetableoil, wheat, salt]\n",
      "4  [blackpepper, shallots, cornflour, cayennepepp...\n"
     ]
    }
   ],
   "source": [
    "# Removing List of Lists and creating additional column with new ingredient list \n",
    "print(new[0:10])\n",
    "new_transposed = zip(new)\n",
    "new = pd.DataFrame(new_transposed)\n",
    "print(new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bakingpowder', 'eggs', 'all-purposeflour', 'raisins', 'milk', 'whitesugar'], ['sugar', 'eggyolks', 'cornstarch', 'creamoftartar', 'bananas', 'vanillawafers', 'milk', 'vanillaextract', 'toastedpecans', 'eggwhites', 'lightrum'], ['sausagelinks', 'fennelbulb', 'fronds', 'oliveoil', 'cubanpeppers', 'onions'], ['meatcuts', 'filepowder', 'smokedsausage', 'okra', 'shrimp', 'andouillesausage', 'water', 'paprika', 'hotsauce', 'garliccloves', 'browning', 'lumpcrabmeat', 'vegetableoil', 'all-purposeflour', 'freshlygroundpepper', 'flatleafparsley', 'bonelesschickenskinlessthigh', 'driedthyme', 'whiterice', 'yellowonion', 'ham'], ['groundblackpepper', 'salt', 'sausagecasings', 'leeks', 'parmigianoreggianocheese', 'cornmeal', 'water', 'extra-virginoliveoil'], ['bakingpowder', 'all-purposeflour', 'peachslices', 'cornstarch', 'heavycream', 'lemonjuice', 'unsaltedbutter', 'salt', 'whitesugar'], ['grapejuice', 'orange', 'whitezinfandel'], ['groundginger', 'whitepepper', 'greenonions', 'orangejuice', 'sugar', 'Sriracha', 'vegetableoil', 'orangezest', 'chickenbroth', 'sesameseeds', 'bonelessskinlesschickenbreasts', 'cornstarch', 'whitevinegar', 'soysauce', 'largeeggs', 'garlic'], ['dicedonions', 'tacoseasoningmix', 'all-purposeflour', 'choppedcilantrofresh', 'groundcumin', 'groundcinnamon', 'vegetableoil', 'bittersweetchocolate', 'choppedgarlic', 'water', 'hotchilipowder', 'bonelessskinlesschickenbreasthalves', 'shreddedMontereyJackcheese', 'chickenbroth', 'Anaheimchile', 'creamcheese', 'driedoregano'], ['eggs', 'cherries', 'dates', 'darkmuscovadosugar', 'groundcinnamon', 'mixedspice', 'cake', 'vanillaextract', 'selfraisingflour', 'sultana', 'rum', 'raisins', 'prunes', 'glacecherries', 'butter', 'port']]\n",
      "                                                   0\n",
      "0  [bakingpowder, eggs, all-purposeflour, raisins...\n",
      "1  [sugar, eggyolks, cornstarch, creamoftartar, b...\n",
      "2  [sausagelinks, fennelbulb, fronds, oliveoil, c...\n",
      "3  [meatcuts, filepowder, smokedsausage, okra, sh...\n",
      "4  [groundblackpepper, salt, sausagecasings, leek...\n"
     ]
    }
   ],
   "source": [
    "print(new_test[0:10])\n",
    "new_test_transposed = zip(new_test)\n",
    "new_test = pd.DataFrame(new_test_transposed)\n",
    "print(new_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conacate additional column with train\n",
    "train = pd.concat([train,new],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test,new_test],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming new column \n",
    "train = train.rename(columns={0: \"ingredients_2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.rename(columns={0: \"ingredients_2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>[romainelettuce, blackolives, grapetomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>[plainflour, groundpepper, salt, tomatoes, gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "\n",
       "                                       ingredients_2  \n",
       "0  [romainelettuce, blackolives, grapetomatoes, g...  \n",
       "1  [plainflour, groundpepper, salt, tomatoes, gro...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>[baking powder, eggs, all-purpose flour, raisi...</td>\n",
       "      <td>[bakingpowder, eggs, all-purposeflour, raisins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>[sugar, egg yolks, corn starch, cream of tarta...</td>\n",
       "      <td>[sugar, eggyolks, cornstarch, creamoftartar, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients  \\\n",
       "0  18009  [baking powder, eggs, all-purpose flour, raisi...   \n",
       "1  28583  [sugar, egg yolks, corn starch, cream of tarta...   \n",
       "\n",
       "                                       ingredients_2  \n",
       "0  [bakingpowder, eggs, all-purposeflour, raisins...  \n",
       "1  [sugar, eggyolks, cornstarch, creamoftartar, b...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas (',' ) from ingredient2 and saving in data in additional column \n",
    "train['ingredients_1'] =train['ingredients_2'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ingredients_1'] =test['ingredients_2'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_2</th>\n",
       "      <th>ingredients_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>[romainelettuce, blackolives, grapetomatoes, g...</td>\n",
       "      <td>romainelettuce blackolives grapetomatoes garli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>[plainflour, groundpepper, salt, tomatoes, gro...</td>\n",
       "      <td>plainflour groundpepper salt tomatoes groundbl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cookingoil, gr...</td>\n",
       "      <td>eggs pepper salt mayonaise cookingoil greenchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>[water, vegetableoil, wheat, salt]</td>\n",
       "      <td>water vegetableoil wheat salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>[blackpepper, shallots, cornflour, cayennepepp...</td>\n",
       "      <td>blackpepper shallots cornflour cayennepepper o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                       ingredients_2  \\\n",
       "0  [romainelettuce, blackolives, grapetomatoes, g...   \n",
       "1  [plainflour, groundpepper, salt, tomatoes, gro...   \n",
       "2  [eggs, pepper, salt, mayonaise, cookingoil, gr...   \n",
       "3                 [water, vegetableoil, wheat, salt]   \n",
       "4  [blackpepper, shallots, cornflour, cayennepepp...   \n",
       "\n",
       "                                       ingredients_1  \n",
       "0  romainelettuce blackolives grapetomatoes garli...  \n",
       "1  plainflour groundpepper salt tomatoes groundbl...  \n",
       "2  eggs pepper salt mayonaise cookingoil greenchi...  \n",
       "3                      water vegetableoil wheat salt  \n",
       "4  blackpepper shallots cornflour cayennepepper o...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_2</th>\n",
       "      <th>ingredients_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>[baking powder, eggs, all-purpose flour, raisi...</td>\n",
       "      <td>[bakingpowder, eggs, all-purposeflour, raisins...</td>\n",
       "      <td>bakingpowder eggs all-purposeflour raisins mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>[sugar, egg yolks, corn starch, cream of tarta...</td>\n",
       "      <td>[sugar, eggyolks, cornstarch, creamoftartar, b...</td>\n",
       "      <td>sugar eggyolks cornstarch creamoftartar banana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients  \\\n",
       "0  18009  [baking powder, eggs, all-purpose flour, raisi...   \n",
       "1  28583  [sugar, egg yolks, corn starch, cream of tarta...   \n",
       "\n",
       "                                       ingredients_2  \\\n",
       "0  [bakingpowder, eggs, all-purposeflour, raisins...   \n",
       "1  [sugar, eggyolks, cornstarch, creamoftartar, b...   \n",
       "\n",
       "                                       ingredients_1  \n",
       "0  bakingpowder eggs all-purposeflour raisins mil...  \n",
       "1  sugar eggyolks cornstarch creamoftartar banana...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split \n",
    "train_corpus = train['ingredients_1']\n",
    "test_corpus = test['ingredients_1']\n",
    "y = train['cuisine']\n",
    "X  = train_corpus\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39774,)\n",
      "(9944,)\n",
      "(27841,)\n",
      "(11933,)\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus.shape)\n",
    "print(test_corpus.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove array\n",
    "#train['ingredients_clean_string'] = [' , '.join(z).strip() for z in train['ingredients']]   \n",
    "#test['ingredients_clean_string'] = [' , '.join(z).strip() for z in test['ingredients']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating TFIDF vector metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf=train_vectorizer.fit_transform(X_train).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27841, 6166)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfidf = train_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11933, 6166)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf=train_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9944, 6166)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.29395641, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11933x6166 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 130091 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9944x6166 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 108817 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model - 1 SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LinearSVC(C=0.8, class_weight=None, dual=False,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.0001, verbose=0),\n",
       "             iid='deprecated', n_jobs=None, param_grid={'C': [1, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LinearSVC(C=0.80, penalty=\"l2\", dual=False)\n",
    "parameters = {'C':[1, 10]}\n",
    "classifier = GridSearchCV(model, parameters)\n",
    "classifier.fit(train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = classifier.predict(train_tfidf)\n",
    "val_pred = classifier.predict(val_tfidf)\n",
    "test_pred = classifier.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.94      0.86      0.90       327\n",
      "     british       0.89      0.78      0.83       578\n",
      "cajun_creole       0.90      0.87      0.89      1046\n",
      "     chinese       0.91      0.94      0.93      1889\n",
      "    filipino       0.94      0.89      0.91       522\n",
      "      french       0.82      0.81      0.82      1834\n",
      "       greek       0.93      0.87      0.90       827\n",
      "      indian       0.93      0.97      0.95      2107\n",
      "       irish       0.91      0.82      0.87       477\n",
      "     italian       0.90      0.96      0.93      5470\n",
      "    jamaican       0.96      0.91      0.94       369\n",
      "    japanese       0.95      0.85      0.90       989\n",
      "      korean       0.97      0.93      0.95       595\n",
      "     mexican       0.96      0.97      0.96      4531\n",
      "    moroccan       0.94      0.93      0.93       569\n",
      "     russian       0.89      0.80      0.85       335\n",
      " southern_us       0.87      0.92      0.89      3015\n",
      "     spanish       0.88      0.71      0.78       699\n",
      "        thai       0.91      0.92      0.92      1093\n",
      "  vietnamese       0.91      0.81      0.86       569\n",
      "\n",
      "    accuracy                           0.91     27841\n",
      "   macro avg       0.92      0.88      0.90     27841\n",
      "weighted avg       0.91      0.91      0.91     27841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.79      0.56      0.65       140\n",
      "     british       0.56      0.43      0.49       226\n",
      "cajun_creole       0.80      0.71      0.75       500\n",
      "     chinese       0.77      0.88      0.82       784\n",
      "    filipino       0.70      0.60      0.65       233\n",
      "      french       0.61      0.62      0.61       812\n",
      "       greek       0.78      0.69      0.73       348\n",
      "      indian       0.86      0.91      0.89       896\n",
      "       irish       0.65      0.51      0.57       190\n",
      "     italian       0.80      0.89      0.84      2368\n",
      "    jamaican       0.82      0.71      0.76       157\n",
      "    japanese       0.83      0.71      0.77       434\n",
      "      korean       0.81      0.75      0.78       235\n",
      "     mexican       0.90      0.93      0.91      1907\n",
      "    moroccan       0.85      0.77      0.81       252\n",
      "     russian       0.66      0.42      0.52       154\n",
      " southern_us       0.73      0.79      0.75      1305\n",
      "     spanish       0.66      0.45      0.54       290\n",
      "        thai       0.77      0.75      0.76       446\n",
      "  vietnamese       0.73      0.51      0.60       256\n",
      "\n",
      "    accuracy                           0.79     11933\n",
      "   macro avg       0.75      0.68      0.71     11933\n",
      "weighted avg       0.78      0.79      0.78     11933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(classification_report(y_test, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = pd.DataFrame(test_pred)\n",
    "sub = pd.concat([test['id'],test_predict],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"svc.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(train_tfidf, y_train)\n",
    "train_pred_rf = rf.predict(train_tfidf)\n",
    "val_pred_rf = rf.predict(val_tfidf)\n",
    "test_pred_rf = rf.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       1.00      1.00      1.00       327\n",
      "     british       1.00      1.00      1.00       578\n",
      "cajun_creole       1.00      1.00      1.00      1046\n",
      "     chinese       1.00      1.00      1.00      1889\n",
      "    filipino       1.00      1.00      1.00       522\n",
      "      french       1.00      1.00      1.00      1834\n",
      "       greek       1.00      1.00      1.00       827\n",
      "      indian       1.00      1.00      1.00      2107\n",
      "       irish       1.00      1.00      1.00       477\n",
      "     italian       1.00      1.00      1.00      5470\n",
      "    jamaican       1.00      1.00      1.00       369\n",
      "    japanese       1.00      1.00      1.00       989\n",
      "      korean       1.00      1.00      1.00       595\n",
      "     mexican       1.00      1.00      1.00      4531\n",
      "    moroccan       1.00      1.00      1.00       569\n",
      "     russian       1.00      1.00      1.00       335\n",
      " southern_us       1.00      1.00      1.00      3015\n",
      "     spanish       1.00      1.00      1.00       699\n",
      "        thai       1.00      1.00      1.00      1093\n",
      "  vietnamese       1.00      1.00      1.00       569\n",
      "\n",
      "    accuracy                           1.00     27841\n",
      "   macro avg       1.00      1.00      1.00     27841\n",
      "weighted avg       1.00      1.00      1.00     27841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.80      0.32      0.46       140\n",
      "     british       0.59      0.18      0.27       226\n",
      "cajun_creole       0.82      0.62      0.71       500\n",
      "     chinese       0.65      0.88      0.75       784\n",
      "    filipino       0.74      0.43      0.54       233\n",
      "      french       0.53      0.45      0.49       812\n",
      "       greek       0.78      0.44      0.56       348\n",
      "      indian       0.82      0.88      0.85       896\n",
      "       irish       0.66      0.23      0.34       190\n",
      "     italian       0.67      0.91      0.77      2368\n",
      "    jamaican       0.93      0.39      0.55       157\n",
      "    japanese       0.85      0.53      0.65       434\n",
      "      korean       0.89      0.54      0.67       235\n",
      "     mexican       0.80      0.92      0.86      1907\n",
      "    moroccan       0.86      0.48      0.62       252\n",
      "     russian       0.92      0.23      0.37       154\n",
      " southern_us       0.59      0.75      0.66      1305\n",
      "     spanish       0.78      0.19      0.31       290\n",
      "        thai       0.76      0.72      0.74       446\n",
      "  vietnamese       0.82      0.36      0.50       256\n",
      "\n",
      "    accuracy                           0.71     11933\n",
      "   macro avg       0.76      0.52      0.58     11933\n",
      "weighted avg       0.72      0.71      0.69     11933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, train_pred_rf))\n",
    "print(classification_report(y_test, val_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "rfc = RandomForestClassifier(n_jobs=-1, max_features='sqrt') \n",
    " \n",
    "#Use a grid over parameters of interest\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [300, 500, 700],\n",
    "           \"max_depth\" : [None, 1, 2, 3],\n",
    "           \"min_samples_leaf\" : [1, 2, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=rfc, param_grid=param_grid,cv=5)\n",
    "clf.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best = RandomForestClassifier(**clf.best_params_)\n",
    "rfc_best.fit(train_tfidf, y_train)\n",
    "train_pred_rfc_best = rfc_best.predict(train_tfidf)\n",
    "val_pred_rfc_best = rfc_best.predict(val_tfidf)\n",
    "test_pred_rfc_best = rfc_best.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       1.00      1.00      1.00       320\n",
      "     british       1.00      1.00      1.00       583\n",
      "cajun_creole       1.00      1.00      1.00      1059\n",
      "     chinese       1.00      1.00      1.00      1846\n",
      "    filipino       1.00      1.00      1.00       525\n",
      "      french       1.00      1.00      1.00      1897\n",
      "       greek       1.00      1.00      1.00       828\n",
      "      indian       1.00      1.00      1.00      2096\n",
      "       irish       1.00      1.00      1.00       476\n",
      "     italian       1.00      1.00      1.00      5479\n",
      "    jamaican       1.00      1.00      1.00       357\n",
      "    japanese       1.00      1.00      1.00       968\n",
      "      korean       1.00      1.00      1.00       574\n",
      "     mexican       1.00      1.00      1.00      4543\n",
      "    moroccan       1.00      1.00      1.00       578\n",
      "     russian       1.00      1.00      1.00       337\n",
      " southern_us       1.00      1.00      1.00      3059\n",
      "     spanish       1.00      1.00      1.00       678\n",
      "        thai       1.00      1.00      1.00      1076\n",
      "  vietnamese       1.00      1.00      1.00       562\n",
      "\n",
      "    accuracy                           1.00     27841\n",
      "   macro avg       1.00      1.00      1.00     27841\n",
      "weighted avg       1.00      1.00      1.00     27841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.89      0.39      0.54       147\n",
      "     british       0.51      0.15      0.24       221\n",
      "cajun_creole       0.79      0.58      0.67       487\n",
      "     chinese       0.67      0.87      0.76       827\n",
      "    filipino       0.79      0.43      0.56       230\n",
      "      french       0.54      0.47      0.50       749\n",
      "       greek       0.79      0.42      0.55       347\n",
      "      indian       0.79      0.88      0.83       907\n",
      "       irish       0.65      0.20      0.31       191\n",
      "     italian       0.67      0.90      0.77      2359\n",
      "    jamaican       0.88      0.30      0.45       169\n",
      "    japanese       0.83      0.56      0.67       455\n",
      "      korean       0.87      0.55      0.68       256\n",
      "     mexican       0.79      0.91      0.84      1895\n",
      "    moroccan       0.84      0.45      0.59       243\n",
      "     russian       0.87      0.22      0.36       152\n",
      " southern_us       0.55      0.74      0.63      1261\n",
      "     spanish       0.73      0.20      0.31       311\n",
      "        thai       0.73      0.67      0.70       463\n",
      "  vietnamese       0.84      0.37      0.51       263\n",
      "\n",
      "    accuracy                           0.70     11933\n",
      "   macro avg       0.75      0.51      0.57     11933\n",
      "weighted avg       0.72      0.70      0.68     11933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, train_pred_rf))\n",
    "print(classification_report(y_test, val_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_rf = pd.DataFrame(test_pred_rf)\n",
    "sub = pd.concat([test['id'],test_predict_rf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"rf.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model 3: Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(shuffle=True, random_state=101)\n",
    "sgd.fit(train_tfidf,y_train)\n",
    "train_pred_sgd = sgd.predict(train_tfidf)\n",
    "val_pred_sgd = sgd.predict(val_tfidf)\n",
    "test_pred_sgd = sgd.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.88      0.53      0.66       320\n",
      "     british       0.90      0.44      0.59       583\n",
      "cajun_creole       0.86      0.76      0.81      1059\n",
      "     chinese       0.84      0.90      0.87      1846\n",
      "    filipino       0.89      0.71      0.79       525\n",
      "      french       0.77      0.64      0.70      1897\n",
      "       greek       0.84      0.76      0.80       828\n",
      "      indian       0.87      0.94      0.90      2096\n",
      "       irish       0.86      0.50      0.63       476\n",
      "     italian       0.77      0.95      0.85      5479\n",
      "    jamaican       0.85      0.76      0.80       357\n",
      "    japanese       0.93      0.72      0.81       968\n",
      "      korean       0.92      0.80      0.86       574\n",
      "     mexican       0.91      0.94      0.92      4543\n",
      "    moroccan       0.91      0.76      0.83       578\n",
      "     russian       0.87      0.52      0.65       337\n",
      " southern_us       0.74      0.87      0.80      3059\n",
      "     spanish       0.88      0.41      0.56       678\n",
      "        thai       0.78      0.86      0.82      1076\n",
      "  vietnamese       0.90      0.48      0.62       562\n",
      "\n",
      "    accuracy                           0.83     27841\n",
      "   macro avg       0.86      0.71      0.76     27841\n",
      "weighted avg       0.83      0.83      0.82     27841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.80      0.46      0.58       147\n",
      "     british       0.61      0.24      0.34       221\n",
      "cajun_creole       0.78      0.68      0.72       487\n",
      "     chinese       0.79      0.86      0.82       827\n",
      "    filipino       0.69      0.55      0.61       230\n",
      "      french       0.64      0.56      0.59       749\n",
      "       greek       0.74      0.64      0.68       347\n",
      "      indian       0.83      0.90      0.87       907\n",
      "       irish       0.72      0.37      0.49       191\n",
      "     italian       0.75      0.91      0.82      2359\n",
      "    jamaican       0.77      0.65      0.71       169\n",
      "    japanese       0.90      0.67      0.77       455\n",
      "      korean       0.84      0.71      0.77       256\n",
      "     mexican       0.88      0.93      0.90      1895\n",
      "    moroccan       0.86      0.71      0.78       243\n",
      "     russian       0.76      0.40      0.53       152\n",
      " southern_us       0.63      0.80      0.71      1261\n",
      "     spanish       0.76      0.35      0.48       311\n",
      "        thai       0.71      0.77      0.74       463\n",
      "  vietnamese       0.82      0.37      0.51       263\n",
      "\n",
      "    accuracy                           0.76     11933\n",
      "   macro avg       0.76      0.63      0.67     11933\n",
      "weighted avg       0.77      0.76      0.75     11933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, train_pred_sgd))\n",
    "print(classification_report(y_test, val_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_sgd = pd.DataFrame(test_pred_sgd)\n",
    "sub = pd.concat([test['id'],test_predict_sgd],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sgd.csv\",index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
